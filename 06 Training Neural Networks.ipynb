{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 CNN Training With Code Example - Neural Network Programming Course\n",
    "\n",
    "## CNN Training Process\n",
    "So far in this series, we learned about Tensors, and we've learned all about PyTorch neural networks. We are now ready to begin the **training process**.\n",
    "* Prepare the data\n",
    "* Build the model\n",
    "* Train the model\n",
    "  * **Calculate the loss, the gradient, and update the weights**\n",
    "* Analyze the model's results\n",
    "\n",
    "## Training: What We Do After The Forward Pass\n",
    "\n",
    "During training, we do a forward pass, but then what? We'll suppose we get a batch and pass it forward through the network. Once the output is obtained, we compare the **predicted output** to the **actual labels**, and once we know **how close** the predicted values are from the actual labels, we **tweak** the weights inside the network in such a way that the values the network predicts move closer to the true values (labels).其实就是通过loss function找最优解  \n",
    "\n",
    "All of this is for **a single batch**, and we **repeat** this process for **every batch** until we have covered every sample in our training set. After we've completed this process for all of the batches and passed over every sample in our **training set**, we say that **an epoch** is complete. We use the word **epoch** to represent a **time period** in which our **entire training** set has been covered.\n",
    "\n",
    "During the **entire training process**, we do as many **epochs** as necessary to reach our desired level of accuracy. With this, we have the following steps:\n",
    "1. Get batch from the training set.\n",
    "2. Pass batch to network.\n",
    "3. Calculate the loss (difference between the predicted values and the true values).\n",
    "4. Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "5. Update the weights using the gradients to reduce the loss.\n",
    "6. Repeat steps 1-5 until one epoch is completed.\n",
    "7. Repeat steps 1-6 for as many epochs required to reach the minimum loss.\n",
    "\n",
    "We already know exactly how to do steps `1` and `2`. We use a loss function to perform step `3`, and you know that we use `backpropagation` and an optimization algorithm to perform step `4` and `5`. Steps `6` and `7` are just standard **Python loops (the training loop)**. Let's see how this is done in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Process\n",
    "\n",
    "Since we disabled PyTorch's gradient tracking feature in a previous episode, we need to be sure to turn it back on (it is on by default).  \n",
    "`torch.set_grad_enabled(True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1b6f9de6e80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120) # Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim = 1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing For The Forward Pass\n",
    "We already know how to get a batch and pass it forward through the network. Let's see what we do after the forward pass is complete.\n",
    "\n",
    "We'll begin by:\n",
    "1. Creating an instance of our `Network` class.\n",
    "2. Creating a data loader that provides batches of size 100 from our training set.\n",
    "3. Unpacking the images and labels from one of these batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch = next(iter(train_loader)) # Getting a batch\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are ready to pass our batch of images forward through the network and obtain the output predictions. Once we have the prediction tensor, we can use the predictions and the true labels to calculate the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating The Loss\n",
    "To do this we will use the `cross_entropy()` loss function that is available in PyTorch's `nn.functional` API. Once we have the loss, we can print it, and also check the number of correct predictions using the function we created a [previous post](https://github.com/unclestrong/DeepLearning-code/blob/master/05%20Neural%20Networks%20and%20PyTorch%20Design-P2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels) # Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.307081460952759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_entropy()` function returned a scalar valued tenor, and so we used the `item()` method to print the `loss` as a Python number. We got `11` out of `100` correct, and since we have `10` prediction classes, this is what we'd expect by guessing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating The Gradients\n",
    "Calculating the gradients is very easy using PyTorch. Since our network is a PyTorch `nn.Module`, PyTorch has created a **computation graph** under the hood. As our tensor flowed forward through our network, all of the computations where added to the graph. The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network's weights.\n",
    "\n",
    "Before we calculate the gradients, let's verify that we **currently** have **no gradients** inside our `conv1` layer. The gradients are tensors that are accessible in the `grad` (short for gradient) attribute of the weight tensor of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To `calculate the gradients`, we call the `backward()` method on the loss tensor, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # Calculating the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the gradients of the loss function have been stored inside weight tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 8.0532e-04,  7.1517e-04,  5.4289e-04,  4.2453e-04,  2.2062e-04],\n",
       "          [ 4.2473e-04,  3.6081e-04,  3.4775e-04,  3.3520e-04,  1.3792e-04],\n",
       "          [ 1.8878e-04,  2.0218e-04,  1.3977e-04,  3.1463e-05, -1.6695e-04],\n",
       "          [ 6.2114e-06,  1.1477e-04,  4.5907e-05, -2.9935e-05, -1.2944e-04],\n",
       "          [-2.1969e-04, -1.8537e-04, -2.5566e-04, -2.2936e-04, -2.4350e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0334e-03, -1.3228e-04, -4.6828e-04,  7.5834e-04,  1.1306e-03],\n",
       "          [ 7.5262e-04, -4.0342e-04, -9.5859e-04,  1.9084e-04,  6.4649e-04],\n",
       "          [ 6.9752e-04, -2.2768e-04, -8.4701e-04,  3.4626e-04,  4.3055e-04],\n",
       "          [ 3.6175e-04, -7.0846e-04, -1.4202e-03, -3.4338e-04, -2.2465e-04],\n",
       "          [ 3.8891e-04, -5.8086e-04, -1.4649e-03, -5.2291e-04, -2.1644e-04]]],\n",
       "\n",
       "\n",
       "        [[[-2.7583e-03, -2.3309e-03, -2.3823e-03, -2.7402e-03, -2.4740e-03],\n",
       "          [-2.3130e-03, -1.8277e-03, -2.0964e-03, -2.7168e-03, -2.2019e-03],\n",
       "          [-2.1739e-03, -1.8778e-03, -2.1596e-03, -2.5166e-03, -1.9841e-03],\n",
       "          [-2.3112e-03, -1.6920e-03, -1.9620e-03, -2.3040e-03, -1.9130e-03],\n",
       "          [-1.9026e-03, -1.5760e-03, -1.6420e-03, -2.1162e-03, -1.5236e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9256e-04,  4.3901e-05, -9.8919e-04, -9.3412e-04, -8.7138e-04],\n",
       "          [ 4.7728e-04,  2.1598e-04, -6.6322e-04, -1.1400e-03, -9.9659e-04],\n",
       "          [ 2.8956e-04,  3.8861e-04, -5.9428e-04, -1.1186e-03, -5.9322e-04],\n",
       "          [ 2.0328e-04,  1.6961e-04, -8.3956e-04, -1.1020e-03, -3.6717e-04],\n",
       "          [ 2.0307e-04, -7.7453e-06, -9.7802e-04, -9.9741e-04, -6.3992e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2192e-04, -1.1592e-04, -5.3961e-04, -8.3908e-04, -1.4125e-03],\n",
       "          [ 5.3511e-04,  6.1669e-04,  4.1265e-05, -3.2095e-04, -1.2030e-03],\n",
       "          [ 8.4875e-04,  9.2720e-04,  4.7815e-04,  2.3308e-05, -9.8388e-04],\n",
       "          [ 7.9033e-04,  9.1618e-04,  3.3051e-04, -1.2498e-04, -9.4174e-04],\n",
       "          [ 5.1729e-04,  7.7212e-04,  3.4925e-04, -1.4285e-04, -1.1027e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7515e-03,  1.5406e-03,  1.8931e-03,  1.3164e-03,  1.5501e-03],\n",
       "          [ 1.4139e-03,  1.6735e-03,  2.0061e-03,  1.6903e-03,  2.0538e-03],\n",
       "          [ 1.1625e-03,  1.5831e-03,  1.9177e-03,  9.5898e-04,  1.8004e-03],\n",
       "          [ 1.0228e-03,  1.3901e-03,  1.5015e-03,  6.2995e-04,  1.4678e-03],\n",
       "          [ 1.0027e-03,  1.0492e-03,  1.3290e-03,  4.3028e-04,  1.2901e-03]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients are used by the optimizer to update the respective weights. To create our optimizer, we use the `torch.optim` package that has many optimization algorithm implementations that we can use. We'll use `Adam` for our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating The Weights\n",
    "To the `Adam` class constructor, we pass the `network parameters` (this is how the optimizer is able to access the gradients), and we pass the `learning rate` .\n",
    "\n",
    "Finally, all we have to do to update the weights is to tell the optimizer to use the gradients to step in the direction of the loss function's minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "optimizer.step() # Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `step()` function is called, the optimizer updates the weights using the gradients that are stored in the network's parameters. This means that we should expect our loss to be reduced if we pass the same batch through the network again. Checking this, we can see that this is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.307081460952759"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2812142372131348"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Using A Single Batch\n",
    "We can summarize the code for training with a single batch in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.300954818725586\n",
      "loss2: 2.2833118438720703\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "optimizer.step() # Update Weights\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 01\n",
    "Q1:During the training process, once the output is obtained, we compare the predicted output to the _______________.<br>\n",
    "A1:labels\n",
    "\n",
    "Q2:Once we know how close the predicted values are to the actual labels, we tweak the weights inside the network in such a way that the predicted values move _______________ the true values (labels).  \n",
    "A2:closer to\n",
    "\n",
    "Q3:After we've completed the training process for all the batches in our training set, we say that _______________ is complete.  \n",
    "A3:an epoch\n",
    "\n",
    "Q4:During the training process, we use the word _______________ to represent a time period for which the entire training set (every batch) has been passed to the network.\n",
    "A4:epoch  \n",
    "\n",
    "Q5:During the entire training process, we do as many epochs as necessary to reach the _______________.<br>\n",
    "A5:minimum loss\n",
    "\n",
    "Q6:To begin the training process, the first step is to get a batch from the training set. What is the second step?  \n",
    "A6:Pass the obtained batch to the network.\n",
    "\n",
    "Q7:During the training process, after we pass a batch to the network, we use the predicted values and the labels to _______________.<br>\n",
    "A7:calculate the loss\n",
    "\n",
    "Q8:PyTorch's gradient tracking feature is turned on using which piece of code?  \n",
    "A8:torch.set_grad_enabled(True)\n",
    "\n",
    "Q9:Which piece of code makes the most sense for creating a PyTorch DataLoader?  \n",
    "A9:torch.utils.data.DataLoader(train_set)\n",
    "\n",
    "Q10:The cross_entropy() loss function lives in which PyTorch package?  \n",
    "A10:torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 CNN Training Loop Explained - Neural Network Code Project\n",
    "## CNN Training Loop - Teach A Neural Network\n",
    "In the last episode, we learned that the [training process](https://deeplizard.com/learn/video/sZAlS3_dnk0) is an iterative process, and to train a neural network, we build what is called the training loop.\n",
    "* Prepare the data\n",
    "* Build the model\n",
    "* Train the model\n",
    "  * Build the training loop\n",
    "* Analyze the model's results\n",
    "\n",
    "### Training With A Single Batch\n",
    "We can summarize the code for training with a single batch in the following way:\n",
    "```python\n",
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "optimizer.step() # Update Weights\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item())\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```python\n",
    "loss1: 2.300954818725586\n",
    "loss2: 2.2833118438720703\n",
    "```\n",
    "\n",
    "One thing that you'll notice is that we get **different results each time** we run this code. This is because the model is created each time at the top, and we know from previous posts that the model weights are **randomly initialized**.\n",
    "\n",
    "### Training With All Batches (Single Epoch)\n",
    "Now, to train with all of the **batches** available inside our **data loader**, we need to make a few changes and add one additional line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 46957 loss: 347.39798778295517\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for batch in train_loader: # Get Batch\n",
    "    images, labels = batch \n",
    "\n",
    "    preds = network(images) # Pass Batch\n",
    "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate Gradients\n",
    "    optimizer.step() # Update Weights\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "print(\n",
    "    \"epoch:\", 0, \n",
    "    \"total_correct:\", total_correct, \n",
    "    \"loss:\", total_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting a single batch from our data loader, we'll create a for loop that will **iterate** over **all of the batches**.\n",
    "\n",
    "Since we have `60,000` samples in our training set, we will have `60,000 / 100 = 600` iterations. For this reason, we'll remove the print statement from within the loop, and keep track of the `total loss` and the `total number` of correct predictions printing them at the end.\n",
    "\n",
    "Something to notice about these `600` iterations is that our `weights` will be `updated 600 times` by the end of the loop. If we **raise the batch_size** this number will **go down** and if we **lower the batch_size** this number will **go up**.\n",
    "\n",
    "Finally, after we call the `backward()` method on our loss tensor, we know the gradients will be calculated and **added** to the `grad` attributes of our network's parameters. For this reason, we need to zero out these gradients. We can do this with a method called `zero_grad()` that comes with the optimizer.\n",
    "\n",
    "We are ready to run this code. This time the code will take longer because the loop is working on `600` batches.\n",
    "\n",
    "```python\n",
    "epoch: 0 total_correct: 46957 loss: 347.39798778295517\n",
    "```\n",
    "\n",
    "We get the results, and we can see that the total number correct out of 60,000 was 46,957."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7826166666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good after only one epoch (a single full pass over the data). Even though we did one epoch, we still have to keep in mind that the **weights** were updated `600` times, and this fact depends on our batch size. If made our batch_batch size larger, say `10,000`, the weights would only be updated `6` times, and the results **wouldn't be quite as good**.\n",
    "\n",
    "### Training With Multiple Epochs\n",
    "To do **multiple epochs**, all we have to do is put this code into a **for loop**. We'll also add the epoch number to the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 46928 loss: 344.27279521524906\n",
      "epoch 1 total_correct: 51277 loss: 232.71748647093773\n",
      "epoch 2 total_correct: 52081 loss: 208.65398114919662\n",
      "epoch 3 total_correct: 52609 loss: 194.9983945786953\n",
      "epoch 4 total_correct: 52906 loss: 190.74674943089485\n",
      "epoch 5 total_correct: 53021 loss: 186.76688426733017\n",
      "epoch 6 total_correct: 53290 loss: 181.0335234105587\n",
      "epoch 7 total_correct: 53226 loss: 180.50387901067734\n",
      "epoch 8 total_correct: 53480 loss: 173.91857013106346\n",
      "epoch 9 total_correct: 53588 loss: 170.3671340867877\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop\n",
    "Putting all of this together, we can pull the `network`, `optimizer`, and the `train_loader` out of the training loop cell.\n",
    "```python\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=100\n",
    "    ,shuffle=True\n",
    ")\n",
    "```\n",
    "This makes it so that we can run the training loop without resetting the networks weights.\n",
    "```python\n",
    "for epoch in range(10):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 02\n",
    "Q1:In the code below, what does the `lr` parameter do?\n",
    "```python\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "```\n",
    "A1:sets the learning rate which tells the optimizer how far to step in the direction of the loss function's minimum\n",
    "\n",
    "Q2:After we call the `backward()` method on our loss tensor, the gradients will be calculated and _______________ of our network's parameters.  \n",
    "A2:added to the grad attributes\n",
    "\n",
    "Q3:Using the code below, determine how many times optimizer.step() will be called during this training loop run.\n",
    "```python\n",
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch\n",
    "        # other stuff happens\n",
    "        optimizer.step() # Update Weights\n",
    "```\n",
    "A3:6000\n",
    "\n",
    "Q4:Suppose we have a fixed training set size. As batch size goes up, which of the following happens inside each epoch?\n",
    "```python\n",
    "for epoch in range(10):\n",
    "    # what happens in here?\n",
    "```\n",
    "A4:the frequency of weight updates goes down\n",
    "\n",
    "Q5:Suppose that our training set contains `60000 `samples. If we are using the data loader below, how many times will our weights be updated during one epoch?\n",
    "```python\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "```\n",
    "A5:6\n",
    "\n",
    "Q6:Suppose that our training set contains `60000` samples. If we are using the data loader below, how many iterations will occur inside our `for batch in train_loader:` loop?\n",
    "```python\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1000)\n",
    "```\n",
    "A6:60\n",
    "\n",
    "Q7:Suppose that our training set contains `60000` samples. If we are using the data loader below, how many iterations will occur inside our `for batch in train_loader:` loop?\n",
    "```python\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "```\n",
    "A7:600\n",
    "\n",
    "Q8:What is the result of running the line of code below?  \n",
    "```python\n",
    "loss.item()\n",
    "```\n",
    "A8:the loss as a Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
