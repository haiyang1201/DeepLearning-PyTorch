{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 CNN Forward Method - PyTorch Deep Learning Implementation\n",
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward()` method is the **actual network transformation**. <br>The forward method is the mapping that maps an **input tensor** to a prediction **output tensor**. Let's see how this is done.\n",
    "\n",
    "This means that the forward method implementation will use **all of the layers** we defined inside the constructor. In this way, the **forward method** explicitly defines the **network's transformation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer 1\n",
    "\n",
    "The **input layer** of any neural network is determined by the **input data**. \n",
    "\n",
    "```python\n",
    "def forward(self,t):\n",
    "    #(1) input ;ayer\n",
    "    t = t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Convolutional Layers: Layers 2 And 3\n",
    "\n",
    "To preform the convolution operation, we pass the tensor to the forward method of the first convolutional layer, `self.conv1`. We've learned how all PyTorch neural network modules have `forward()` methods, and when we call the `forward()` method of a `nn.Module`, there is a special way that we make the call.\n",
    "\n",
    "When want to call the `forward()` method of a `nn.Module` instance, we call the actual instance instead of calling the `forward()` method directly.\n",
    "\n",
    "Instead of doing this `self.conv1.forward(tensor)`, we do this `self.conv1(tensor)`.\n",
    "\n",
    "```python\n",
    "# (2) hidden conv layer\n",
    "t = self.conv1(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "# (3) hidden conv layer\n",
    "t = self.conv2(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "As we can see here, our input tensor is transformed as we move through the convolutional layers. The first convolutional layer has a convolutional operation, followed by a **relu activation** operation whose output is then passed to a max pooling operation with `kernel_size=2` and `stride=2`.(其中kernel size就是filter的大小)\n",
    "\n",
    "The output tensor `t` of the first convolutional layer is then passed to the next convolutional layer, which is identical except for the fact that we call `self.conv2()` instead of `self.conv1()`.\n",
    "\n",
    "Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the `nn.Conv2d()` class instance. The `relu()` and the `max_pool2d()` calls are just **pure operations**. Neither of these have weights, and this is why we call them directly from the `nn.functional` API.\n",
    "\n",
    "Sometimes we may see `pooling operations` referred to as pooling layers. Sometimes we may even hear activation operations called activation layers.\n",
    "\n",
    "**However**, what makes a layer **distinct** from an operation is that layers have **weights**. Since `pooling` operations and `activation` functions **do not have weights**, we will refer to them as **operations** and view them as being added to the collection of layer operations.\n",
    "\n",
    "**确定一个操作能不能叫做layer就看他有没有weights，不过也别被这些术语搞糊涂了，我们就是通过一系列方法的组合来实现这个`forward()`**\n",
    "\n",
    "Don't let these terms confuse the fact that the whole network is simply a composition of functions, and what we are doing now is defining this composition inside our `forward()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Linear Layers: Layers 4 And 5\n",
    "Before we pass our input to the first hidden linear layer, we must `reshape()` or `flatten` our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer.\n",
    "\n",
    "Since the forth layer is the first linear layer, we will include our reshaping operation as a part of the forth layer.\n",
    "\n",
    "```python\n",
    "# (4) hidden linear layer\n",
    "t = t.reshape(-1, 12 * 4 * 4)\n",
    "t = self.fc1(t)\n",
    "t = F.relu(t)\n",
    "\n",
    "# (5) hidden linear layer\n",
    "t = self.fc2(t)\n",
    "t = F.relu(t)\n",
    "```\n",
    "\n",
    "number `12` in the reshaping operation is determined by the number of output channels coming from the previous convolutional layer(`out_channels=12`).\n",
    "\n",
    "The `4 * 4` is actually the height and width of each of the 12 output channels.\n",
    "\n",
    "The height and width dimensions have been reduced from `28 x 28` to `4 x 4` by the convolution and pooling operations.\n",
    "\n",
    "After the tensor is reshaped, we pass the `flattened` tensor to the linear layer and pass this result to the `relu() activation function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer 6\n",
    "\n",
    "The sixth and last layer of our network is a linear layer we call the **output layer**. When we pass our tensor to the output layer, the result will be the **prediction tensor**. Since our data has ten prediction classes, we know our output tensor will have ten elements.\n",
    "\n",
    "```python\n",
    "# (6) output layer\n",
    "t = self.out(t)\n",
    "#t = F.softmax(t, dim=1)\n",
    "```\n",
    "Inside the network we usually use `relu()` as our `non-linear activation function`, but for the output layer, whenever we have a single category that we are trying to predict, we use `softmax()`. The `softmax function` returns a positive probability for each of the prediction classes, and the probabilities sum to `1`.\n",
    "\n",
    "**However**, in our case, we won't use `softmax()` because the loss function that we'll use, `F.cross_entropy()`, implicitly performs the `softmax()` operation on its input, so we'll just return the result of the last linear transformation.\n",
    "\n",
    "The implication of this is that our network will be trained using the softmax operation but will not need to compute the additional operation when the network is used for inference after the training process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 05\n",
    "Q1:In neural network programming, the forward method of a network instance explicitly defines the network's ______________<br>\n",
    "A1:transformation\n",
    "\n",
    "Q2:In neural network programming, the forward method of a network instance is the mapping that maps an input tensor to a prediction output tensor.<br>\n",
    "A2:**True**\n",
    "\n",
    "Q3:The input layer of any neural network is determined by the input data. For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function,$$f(x)=x$$\n",
    "A3:**True**\n",
    "\n",
    "Q4:In neural network programming, all layers that are not the input or output layers are called hidden layers.<br>\n",
    "A4:**True**\n",
    "\n",
    "Q5:In neural network programming, the operations that are defined using _______________ are called layers.<br>\n",
    "A5:weights\n",
    "\n",
    "Q6:In the most general sense, neural networks are mathematical functions. Terms like layers, activation functions, and weights, are just used to help describe the different parts.<br>\n",
    "A6:**True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
