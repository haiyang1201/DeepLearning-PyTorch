{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 CNN Forward Method - PyTorch Deep Learning Implementation\n",
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward()` method is the **actual network transformation**. <br>The forward method is the mapping that maps an **input tensor** to a prediction **output tensor**. Let's see how this is done.\n",
    "\n",
    "This means that the forward method implementation will use **all of the layers** we defined inside the constructor. In this way, the **forward method** explicitly defines the **network's transformation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer 1\n",
    "\n",
    "The **input layer** of any neural network is determined by the **input data**. \n",
    "\n",
    "```python\n",
    "def forward(self,t):\n",
    "    #(1) input ;ayer\n",
    "    t = t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Convolutional Layers: Layers 2 And 3\n",
    "\n",
    "To preform the convolution operation, we pass the tensor to the forward method of the first convolutional layer, `self.conv1`. We've learned how all PyTorch neural network modules have `forward()` methods, and when we call the `forward()` method of a `nn.Module`, there is a special way that we make the call.\n",
    "\n",
    "When want to call the `forward()` method of a `nn.Module` instance, we call the actual instance instead of calling the `forward()` method directly.\n",
    "\n",
    "Instead of doing this `self.conv1.forward(tensor)`, we do this `self.conv1(tensor)`.\n",
    "\n",
    "```python\n",
    "# (2) hidden conv layer\n",
    "t = self.conv1(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "# (3) hidden conv layer\n",
    "t = self.conv2(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "As we can see here, our input tensor is transformed as we move through the convolutional layers. The first convolutional layer has a convolutional operation, followed by a **relu activation** operation whose output is then passed to a max pooling operation with `kernel_size=2` and `stride=2`.(其中kernel size就是filter的大小)\n",
    "\n",
    "The output tensor `t` of the first convolutional layer is then passed to the next convolutional layer, which is identical except for the fact that we call `self.conv2()` instead of `self.conv1()`.\n",
    "\n",
    "Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the `nn.Conv2d()` class instance. The `relu()` and the `max_pool2d()` calls are just **pure operations**. Neither of these have weights, and this is why we call them directly from the `nn.functional` API.\n",
    "\n",
    "Sometimes we may see `pooling operations` referred to as pooling layers. Sometimes we may even hear activation operations called activation layers.\n",
    "\n",
    "**However**, what makes a layer **distinct** from an operation is that layers have **weights**. Since `pooling` operations and `activation` functions **do not have weights**, we will refer to them as **operations** and view them as being added to the collection of layer operations.\n",
    "\n",
    "**确定一个操作能不能叫做layer就看他有没有weights，不过也别被这些术语搞糊涂了，我们就是通过一系列方法的组合来实现这个`forward()`**\n",
    "\n",
    "Don't let these terms confuse the fact that the whole network is simply a composition of functions, and what we are doing now is defining this composition inside our `forward()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Linear Layers: Layers 4 And 5\n",
    "Before we pass our input to the first hidden linear layer, we must `reshape()` or `flatten` our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer.\n",
    "\n",
    "Since the forth layer is the first linear layer, we will include our reshaping operation as a part of the forth layer.\n",
    "\n",
    "```python\n",
    "# (4) hidden linear layer\n",
    "t = t.reshape(-1, 12 * 4 * 4)\n",
    "t = self.fc1(t)\n",
    "t = F.relu(t)\n",
    "\n",
    "# (5) hidden linear layer\n",
    "t = self.fc2(t)\n",
    "t = F.relu(t)\n",
    "```\n",
    "\n",
    "number `12` in the reshaping operation is determined by the number of output channels coming from the previous convolutional layer(`out_channels=12`).\n",
    "\n",
    "The `4 * 4` is actually the height and width of each of the 12 output channels.\n",
    "\n",
    "The height and width dimensions have been reduced from `28 x 28` to `4 x 4` by the convolution and pooling operations.\n",
    "\n",
    "After the tensor is reshaped, we pass the `flattened` tensor to the linear layer and pass this result to the `relu() activation function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer 6\n",
    "\n",
    "The sixth and last layer of our network is a linear layer we call the **output layer**. When we pass our tensor to the output layer, the result will be the **prediction tensor**. Since our data has ten prediction classes, we know our output tensor will have ten elements.\n",
    "\n",
    "```python\n",
    "# (6) output layer\n",
    "t = self.out(t)\n",
    "#t = F.softmax(t, dim=1)\n",
    "```\n",
    "Inside the network we usually use `relu()` as our `non-linear activation function`, but for the output layer, whenever we have a single category that we are trying to predict, we use `softmax()`. The `softmax function` returns a positive probability for each of the prediction classes, and the probabilities sum to `1`.\n",
    "\n",
    "**However**, in our case, we won't use `softmax()` because the loss function that we'll use, `F.cross_entropy()`, implicitly performs the `softmax()` operation on its input, so we'll just return the result of the last linear transformation.\n",
    "\n",
    "The implication of this is that our network will be trained using the softmax operation but will not need to compute the additional operation when the network is used for inference after the training process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 05\n",
    "Q1:In neural network programming, the forward method of a network instance explicitly defines the network's ______________<br>\n",
    "A1:transformation\n",
    "\n",
    "Q2:In neural network programming, the forward method of a network instance is the mapping that maps an input tensor to a prediction output tensor.<br>\n",
    "A2:**True**\n",
    "\n",
    "Q3:The input layer of any neural network is determined by the input data. For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function,$$f(x)=x$$\n",
    "A3:**True**\n",
    "\n",
    "Q4:In neural network programming, all layers that are not the input or output layers are called hidden layers.<br>\n",
    "A4:**True**\n",
    "\n",
    "Q5:In neural network programming, the operations that are defined using _______________ are called layers.<br>\n",
    "A5:weights\n",
    "\n",
    "Q6:In the most general sense, neural networks are mathematical functions. Terms like layers, activation functions, and weights, are just used to help describe the different parts.<br>\n",
    "A6:**True**\n",
    "\n",
    "[Reference](https://deeplizard.com/learn/video/MasG7tZj-hw)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 CNN Image Prediction With PyTorch - Forward Propagation Explained\n",
    "\n",
    "## What Is Forward Propagation?\n",
    "\n",
    "*Forward propagation* is the process of transforming an input tensor to an output tensor. <br>\n",
    "At its core, a neural network is a function that maps an input tensor to an output tensor, and *forward propagation* is just a special name for the process of **passing an input to the network and receiving the output** from the network.\n",
    "\n",
    "For our network, what this means is simply passing our input tensor to the network and receiving the output tensor. To do this, we pass our sample data to the network's `forward()` method.\n",
    "\n",
    "This is why, the `forward()` method has the name *forward*, the execution of **the `forward()` is the process of forward propagation**.\n",
    "\n",
    "The word *forward*, is pretty *straight forward*. ;)\n",
    "\n",
    "However, the word *propagate* means to move or transmit *through some medium*. In the case of **neural networks**, data propagates through the **layers of the network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1,out_channels = 6,kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features=60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features=10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t, kernel_size = 2,stride = 2)\n",
    "        \n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size = 2,stride = 2)\n",
    "        \n",
    "        t = F.relu(self.fc1(t.reshape(-1,12*4*4)))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting With The Network: Forward Pass\n",
    "\n",
    "Before we being, we are going to **turn off PyTorch’s gradient calculation feature**. This will stop PyTorch from automatically building a computation graph as our tensor flows through the network.\n",
    "![pytorch neuron](https://deeplizard.com/images/neural%20network%202%203%202.png)\n",
    "\n",
    "The computation graph keeps track of the network's mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the network’s weights.\n",
    "\n",
    "**Since we are not training the network yet, we aren’t planning on updating the weights, and so we don’t require gradient calculations. We will turn this back on when training begins.**\n",
    "\n",
    "This process of tracking calculations happens in real-time, as the calculations occur.Turning it off isn’t strictly necessary but having the feature turned off does reduce memory consumption since the graph isn't stored in memory. This code will turn the feature off.\n",
    "```python\n",
    "torch.set_grad_enabled(False) \n",
    "```\n",
    "\n",
    "### Passing A Single Image To The Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x216a5bc1ca0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll procure a **single** sample from our training set, unpack the image and the label, and verify the image’s shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_set))\n",
    "image,label = sample\n",
    "image.shape\n",
    "#The image tensor’s shape indicates that we have a single channel image \n",
    "#that is 28 in height and 28 in width. Cool, this is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there's a second step we must preform before simply passing this tensor to our network. When we pass a tensor to our network, the network is expecting a **batch**, so even if we want to pass a single image, we still need a batch.\n",
    "\n",
    "This is no problem. We can create **a batch that contains a single image**. All of this will be packaged into a single **four dimensional tensor** that reflects the following dimensions.\n",
    "\n",
    "This requirement of the network **arises from** the fact that the `forward()` method's in the `nn.Conv2d` convolutional layer classes expect their tenors to have **4 dimensions**. This is pretty standard as most neural network implementations deal with batches of input samples rather than single samples.\n",
    "\n",
    "To put our single sample image tensor into a batch with a size of 1, we just need to `unsqueeze()` the tensor to add an additional dimension. [squeeze tutorial](https://deeplizard.com/learn/video/fCVuiW9AFzY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts an additional dimension that represents a batch of size 1\n",
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = network(image.unsqueeze(0)) \n",
    "# image shape needs to be (batch_size × in_channels × H × W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0207, -0.0841, -0.0833,  0.1248, -0.0734, -0.0905, -0.0102, -0.0013,  0.0937, -0.0687]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we did it! We've used our forward method to get a prediction from the network. The network has returned a prediction tensor that contains a prediction value for each of the ten categories of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could just the `softmax()` function from the `nn.functional` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1036, 0.0933, 0.0933, 0.1149, 0.0943, 0.0927, 0.1004, 0.1013, 0.1114, 0.0947]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print( pred.argmax(dim=1))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label for the first image in our training set is `9`, and using the `argmax()` function we can see that the highest value in our prediction tensor occurred at the class represented by index `3`.\n",
    "\n",
    "The prediction in this case is **incorrect**, **which is what we expect** because the weights in the network were generated randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0207, -0.0841, -0.0833,  0.1248, -0.0734, -0.0905, -0.0102, -0.0013,  0.0937, -0.0687]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Weights Are Randomly Generated\n",
    "There are a couple of important things we need to point out about these results. Most of the probabilities came in close to `10%`, and this `makes sense` because our network is guessing and we have ten prediction classes coming from a `balanced dataset`.\n",
    "\n",
    "Another implication of the randomly generated weights is that **each time we create a new instance of our network**, the **weights** within the network will be **different**. This means that the predictions we get will be different if we create different networks. Keep this in mind. Your predictions will be different from what we see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Network()\n",
    "net2 = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0299,  0.0675, -0.0123,  0.0595,  0.0621,  0.0469, -0.1198,  0.0559,  0.1360,  0.1458]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0902,  0.0108,  0.0641,  0.0207, -0.0602,  0.0682, -0.0044, -0.0218,  0.0343,  0.0474]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 06\n",
    "Q1:_____________ is the process of transforming an input tensor to an output tensor.<br>A1:Forward propagation\n",
    "\n",
    "Q2:The concept of forward propagation is used to indicate that the input tensor data is transmitted through the network in the backward direction.<br>\n",
    "A2:**False**\n",
    "\n",
    "Q3:In neural network programming, the computational graph keeps track of the network's mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the network’s ______________<br>\n",
    "A3:weights\n",
    "\n",
    "Q4:Suppose we have a balanced dataset with ten different classes. Choose an image from this dataset and pass the image to a CNN that has randomly initialized weights. In this situation, what is the approximate prediction probability should we expect to see across all the prediction classes?<br>\n",
    "A4:10%\n",
    "\n",
    "Q5:Suppose we have the pred tensor below. What will be the output of the sum() of the softmax().\n",
    "```python\n",
    "> pred\n",
    "tensor([[0.0991, 0.0916, 0.0907, 0.0949, 0.1013, 0.0922, 0.0990, 0.1130, 0.1107, 0.1074]])\n",
    "\n",
    "> F.softmax(pred, dim=1).sum()\n",
    "```\n",
    "A5:tensor(1.)\n",
    "\n",
    "[Reference](https://deeplizard.com/learn/video/6vweQjouLEE)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Neural Network Batch Processing - Pass Image Batch To PyTorch CNN\n",
    "\n",
    "## Neural Network Batch Processing With PyTorch\n",
    "\n",
    "### Passing A Batch Of Images To The Network\n",
    "We need the following:\n",
    "* Our imports.\n",
    "* Our training set.\n",
    "* Our Network class definition.\n",
    "* To disable gradient tracking. (optional)\n",
    "* A Network class instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4,out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120,out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60,out_features = 10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x216a5bc18b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ll use our training set to create a new `DataLoader` instance, and we’ll set our `batch_size=10`, so the outputs will be more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set,batch_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "images,labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us **two tensors**, a tensor of **images** and a tensor of **corresponding labels**.\n",
    "\n",
    "In the last episode, when we pulled a single image from our training set, we had to `unsqueeze()` the tensor to add another dimension that would effectively transform the singleton image into a batch with a size of one. Now that we are working with the *data loader*, we are dealing with batches by default, so there is no further processing needed.\n",
    "\n",
    "The data loader returns a batch of images that are packaged into a single tensor with a shape that reflects the following axes.\n",
    "\n",
    "`(batch size, input channels, height, width)`\n",
    "\n",
    "This means tensor's shape is in **good shape**, and there's **no need to unsqueeze it**. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first axis of the image tensor tells us that we have a **batch of ten images**. These ten images have **a single color channel** with a **height** and **width** of twenty-eight.\n",
    "\n",
    "The labels tensor has a single axis with a shape of ten, which corresponds to the **ten images** inside our batch. **One label for each image**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6467e-02, -3.2971e-02, -1.4058e-01, -1.2443e-01,  9.6093e-02, -1.4053e-01, -4.4303e-02, -9.8937e-03,\n",
       "         -4.1751e-02, -1.5865e-03],\n",
       "        [ 1.1554e-02, -3.5275e-02, -1.4306e-01, -1.2657e-01,  9.1447e-02, -1.5218e-01, -3.6254e-02, -1.4780e-02,\n",
       "         -3.2344e-02, -6.0051e-03],\n",
       "        [ 2.5395e-02, -5.6169e-02, -1.3118e-01, -1.3064e-01,  1.0796e-01, -1.2940e-01, -4.1606e-02, -2.5502e-02,\n",
       "         -4.0450e-02,  3.5781e-03],\n",
       "        [ 1.9997e-02, -4.8922e-02, -1.3441e-01, -1.2610e-01,  1.0554e-01, -1.3486e-01, -4.2117e-02, -2.1434e-02,\n",
       "         -3.6143e-02,  7.5589e-04],\n",
       "        [ 9.6969e-04, -4.9612e-02, -1.3271e-01, -1.2524e-01,  1.0999e-01, -1.4848e-01, -4.4680e-02, -1.5122e-02,\n",
       "         -3.4455e-02,  3.3246e-03],\n",
       "        [ 2.5426e-02, -3.6826e-02, -1.3773e-01, -1.2568e-01,  9.1324e-02, -1.4248e-01, -4.1219e-02, -1.4956e-02,\n",
       "         -3.4798e-02, -1.1648e-02],\n",
       "        [ 1.8192e-02, -3.0211e-02, -1.4076e-01, -1.2476e-01,  9.3960e-02, -1.2872e-01, -4.6744e-02, -1.7168e-02,\n",
       "         -3.7452e-02,  1.4616e-04],\n",
       "        [ 1.4099e-02, -2.6243e-02, -1.4466e-01, -1.1376e-01,  9.1605e-02, -1.4882e-01, -4.2618e-02, -8.7141e-03,\n",
       "         -3.0565e-02, -1.3770e-02],\n",
       "        [ 3.0475e-02, -4.5589e-02, -1.3294e-01, -1.2824e-01,  1.0284e-01, -1.2926e-01, -4.4715e-02, -1.6938e-02,\n",
       "         -4.4489e-02, -1.3020e-04],\n",
       "        [ 3.0999e-02, -3.5737e-02, -1.4048e-01, -1.2686e-01,  9.4163e-02, -1.3359e-01, -4.3089e-02, -1.7906e-02,\n",
       "         -3.5539e-02, -9.2995e-03]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction tensor has a shape of `10 by 10`, which gives us two axes that each have a length of ten. This reflects the fact that we have ten images and for each of these ten images we have ten prediction classes.\n",
    "\n",
    "`(batch size, number of prediction classes)`\n",
    "\n",
    "The elements of the **first dimension** are arrays of length ten. Each of these array elements contain the **ten predictions** for each category for the corresponding image.\n",
    "\n",
    "The elements of the **second dimension** are **numbers**. Each number is the assigned value of the specific output class. The output classes are encoded by the indexes, so each index represents a specific output class. This mapping is given by this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Argmax: Prediction Vs Label\n",
    "\n",
    "To check the predictions against the labels, we use the `argmax()` function to figure out which index contains the **highest prediction value**. Once we know which index has the highest prediction value, we can **compare the index with the label** to see if there is a **match**.\n",
    "\n",
    "To do this, we call the `argmax()` function on the prediction tensor, and we specify **second dimension**.\n",
    "\n",
    "The **second dimension** is the **last dimension** of our prediction tensor. Remember from all of our work on tensors, the last dimension of a tensor always contains **numbers** while every other dimension contains other **smaller tensor**.\n",
    "\n",
    "一个tensor的最后一维就是一个具体数字，再往高维走就是一个tensor\n",
    "\n",
    "In our prediction tensor's case, we have ten groups of numbers. What the `argmax()` function is doing is looking inside each of these ten groups, finding the max value, and outputting its index.\n",
    "\n",
    "For each group of ten numbers:\n",
    "* Find max value.\n",
    "* Output index\n",
    "\n",
    "The interpretation of this is that, for each of the images in the batch, we are finding the prediction class that has the **highest value**. This is the category the network is **predicting most strongly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the `argmax()` function is a tensor of ten prediction categories. Each number is the index where the highest value occurred. We have ten numbers because there were ten images. Once we have this tensor of indices of highest values, we can compare it against the label tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve the comparison, we are using the `eq()` function. The `eq()` function computes an element-wise equals operation between the argmax output and the `labels` tensor.\n",
    "\n",
    "This returns `True` if the prediction category in the argmax output matches the label and `False` otherwise.\n",
    "\n",
    "Finally, if we call the `sum()` function on this result, we can reduce the output into a single number of correct predictions inside this scalar valued tensor.\n",
    "\n",
    "We can wrap this last call into a function called `get_num_correct()` that accepts the predictions and the labels, and uses the `item()` method to return the Python number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 07\n",
    "\n",
    "Q1:In many neural network APIs, networks accept batches of images as input.<br>\n",
    "A1:**True**\n",
    "\n",
    "Q2:In neural network programming, when we pull a single image from the training set, we unsqueeze the image tensor to add another dimension so that the tensor is transformed into a ______________<br>\n",
    "A2:batch tensor with one image\n",
    "\n",
    "Q3:Suppose we have a label tensor with a shape of ten. The value of ten in the shape corresponds to the ten images inside the _______________. One label for each image.<br>\n",
    "A3:batch\n",
    "\n",
    "Q4:Suppose the prediction tensor output from a network has a shape of ten by ten. This information tells us that we have a batch of ten images and ten prediction classes in the training set.<br>\n",
    "```python\n",
    "> preds.shape\n",
    "torch.Size([10, 10])\n",
    "```\n",
    "A4:**True**\n",
    "\n",
    "Q5:When we call the argmax() function on a prediction tensor, the argmax() function looks inside each of the ten prediction groups, finds the max value, and outputs the value.<br>\n",
    "```python\n",
    "> preds.argmax(dim=1)\n",
    "tensor([5, 5, 5, 5, 5, 5, 4, 5, 5, 4])\n",
    "```\n",
    "A5:**False**\n",
    "\n",
    "Q6:When we call the `argmax()` function on a prediction tensor, the `argmax()` function looks inside each of the ten prediction groups, finds the max value, and outputs the index location of the max value.<br>\n",
    "A6:**True**\n",
    "\n",
    "[Reference](https://deeplizard.com/learn/video/p1xZ2yWU1eo)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
