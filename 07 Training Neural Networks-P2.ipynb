{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Stack Vs Concat In PyTorch, TensorFlow & NumPy - Deep Learning Tensor Ops\n",
    "\n",
    "## Tensor Ops For Deep Learning: Concatenate Vs Stack\n",
    "**In this episode, we will dissect the difference between concatenating and stacking tensors together. We’ll look at two examples, one with PyTorch, one with NumPy.**\n",
    "\n",
    "## Existing Vs New Axes\n",
    "The **difference** between **stacking** and **concatenating** tensors can be described in a single sentence, so here goes.\n",
    "\n",
    "***Concatenating* joins a sequence of tensors along an *existing axis*, and *stacking* joins a sequence of tensors along a *new axis***.\n",
    "\n",
    "And that’s all there is to it!\n",
    "\n",
    "This is the **difference** between stacking and concatenating. However, the description here is kind of tricky, so let’s look at some **examples** to get a handle on what exactly how this can be better understood. We’ll look at stacking and concatenating in PyTorch, TensorFlow, and NumPy. Let’s do it.\n",
    "\n",
    "For the most part, **concatenating** along an existing axis of a tensor is pretty **straight forward**. The confusion usually arises when we want to **concat along a new axis**. For this we **stack**. Another way of saying that we stack is to say that we **create a new axis and then concat on that axis**.\n",
    "\n",
    "| **Join Method** | **Where** |\n",
    "| -- | -- |\n",
    "| Concatenate | Along an existing axis |\n",
    "| Stack | Along a new axis|\n",
    "\n",
    "For this reason, let’s be sure we know how to **create a new axis** for a given tensor, and then we’ll start stacking and concatenating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Add Or Insert An Axis Into A Tensor\n",
    "To demonstrate this idea of adding an axis, we’ll use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we’re importing PyTorch and creating a simple tensor that has **a single axis** of **length three**. Now, to **add an axis** to a tensor in PyTorch, we use the `unsqueeze()` function. Note that this is the opposite of squeezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are we are **adding an axis**, a.k.a dimension at index zero of this tensor. This gives us a tensor with a shape of `1 x 3`. When we say index **zero of the tensor**, we mean the **first index** of the tensor's shape.\n",
    "\n",
    "Now, we can also **add an axis** at the **second index** of this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a tensor with a shape of `3 x 1`. Adding axes like this changes the way the data is organized inside the tensor, but it **does not change the data itself**. Basically, we are just **reshaping** the tensor. We can see that by checking the shape of each one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t1.unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t1.unsqueeze(dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, thinking back about concatenating verses stacking, when we **concat**, we are **joining a sequence of tensors along an existing axis**. This means that we are **extending the length of an existing axis**.\n",
    "\n",
    "When we **stack**, we are **creating a new axis** that didn’t exist before and this happens across all the tensors in our sequence, and then we **concat along this new sequence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Vs Cat In PyTorch\n",
    "\n",
    "With PyTorch the two functions we use for these operations are `stack` and `cat`. Let’s create a sequence of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1,1,1])\n",
    "t2 = torch.tensor([2,2,2])\n",
    "t3 = torch.tensor([3,3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s **concatenate** these with one another. Notice that each of these tensors have **a single axis**. This means that the **result** of the cat function will also have **a single axis**. This is because when we concatenate, we do it along an **existing axis**. Notice that in this example, the only existing axis is the **first axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we took **three single axis** tensors each having an axis length of three, and now we have a single tensor with **an axis length of nine**.依旧是一个中括号，维度没有增加\n",
    "\n",
    "Now, let’s **stack** these tensors along a new axis that we’ll insert. We’ll insert an axis at the first index. Note that this insertion will be happening implicitly under the hood by the stack function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a new tensor that has a shape of `3 x 3`. Notice how the three tensors are concatenated along the **first axis** of this tensor. Note that we can also **insert the new axis** explicitly, and preform the concatenation directly.现在的tensor变为了两个中括号，维度增加了一维\n",
    "\n",
    "\n",
    "To see that this statement is true. Let’s **add a new axis** of length one to all of our tensors by **unsqueezing** them and then, **cat** along the **first axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (\n",
    "         t1.unsqueeze(0)\n",
    "        ,t2.unsqueeze(0)\n",
    "        ,t3.unsqueeze(0)\n",
    "    )\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that we get the **same result** that we got by **stacking**. However, the call to **stack** was **much cleaner** because the new axis insertion was handed by the stack function.\n",
    "\n",
    "**Concatenation happens along an existing axis.**\n",
    "\n",
    "Note that we **cannot** concat this sequence of tensors along the **second axis** because there currently is **no second axis in existence**, so in this case, stacking is our only option.\n",
    "\n",
    "Let’s try stacking along the second axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright, we stack with respect to the second axis and this is the result.\n",
    "torch.cat(\n",
    "    (\n",
    "         t1.unsqueeze(1)\n",
    "        ,t2.unsqueeze(1)\n",
    "        ,t3.unsqueeze(1)\n",
    "    )\n",
    "    ,dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this result, think back to what it looked like when we inserted a new axis at the end of the tensor. Now, we just do that to all our tensors, and we can cat them like so along the second axis. Inspecting the unsqueezed outputs can help make this solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [3],\n",
       "        [3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Vs Concatenate In NumPy\n",
    "Let's work with NumPy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t1 = np.array([1,1,1])\n",
    "t2 = np.array([2,2,2])\n",
    "t3 = np.array([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we've created our three tensors. Now, let's concatenate them with one another.\n",
    "np.concatenate(\n",
    "    (t1,t2,t3)\n",
    "    ,axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this gives us what we expect. Note that NumPy also used the axis parameter name, but here, we are also seeing another naming variation. NumPy uses the full word concatenate as the function name.\n",
    "\n",
    "| **Library** | **Function Name** |\n",
    "| -- | -- |\n",
    "| PyTorch | `cat()` |\n",
    "| NumPy | `concatenate() ` |\n",
    "\n",
    "Okay, let's stack now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 2, 2],\n",
       "       [3, 3, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 2, 2],\n",
       "       [3, 3, 3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As expected, the result is a rank-2 tensor with a shape of 3 x 3. Now, \n",
    "# we'll try the manual way.\n",
    "np.concatenate(\n",
    "    (\n",
    "         np.expand_dims(t1, 0)\n",
    "        ,np.expand_dims(t2, 0)\n",
    "        ,np.expand_dims(t3, 0)\n",
    "    )\n",
    "    ,axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result is the **same** as when we used the `stack()` function. Additionally, observe that NumPy also uses the term `expand dims` for the function name.\n",
    "\n",
    "Now, we'll finish this off by stacking using the **second axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And, with manual insertion.\n",
    "np.concatenate(\n",
    "    (\n",
    "         np.expand_dims(t1, 1)\n",
    "        ,np.expand_dims(t2, 1)\n",
    "        ,np.expand_dims(t3, 1)\n",
    "    )\n",
    "    ,axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Or Concat: Real-Life Examples\n",
    "Here are three **concrete examples** that we can encounter in real life. Let’s decide when we need to **stack** and when we need to **concat**.\n",
    "\n",
    "### Joining Images Into A Single Batch\n",
    "Suppose we have **three individual images** as tensors. Each image tensor has **three dimensions**, a *channel axis*, a *height axis*, a *width axis*. Note that each of these tensors are **separate** from one another. Now, assume that our task is to join these tensors together to form **a single batch** tensor of three images.\n",
    "\n",
    "*Do we concat or do we stack?*\n",
    "\n",
    "Well, notice that in this example, there are **only three dimensions** in existence, and for a batch, we need **four dimensions**. This means that the answer is to **stack** the tensors along a **new axis**. This new axis will be the **batch axis**. This will give us a single tensor with four dimensions by adding one for the batch.\n",
    "\n",
    "Note that **if** we join these three along any of the **existing dimensions**, we would be **messing up** either the channels, the height, or the width. We don’t want to mess our data up like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.zeros(3,28,28)\n",
    "t2 = torch.zeros(3,28,28)\n",
    "t3 = torch.zeros(3,28,28)\n",
    "\n",
    "torch.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ").shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Batches Into A Single Batch\n",
    "Now, suppose we have the same **three images** as before, but this time the images **already have a dimension for the batch**. This actually means we have **three batches of size one**. Assume that it is our task to obtain **a single batch of three images**.\n",
    "\n",
    "*Do we concat or stack?*\n",
    "\n",
    "Well, notice how there is an **existing dimension** that we can concat on. This means that we concat these along the batch dimension. In this case there is **no need to stack**.\n",
    "\n",
    "Here is a code example of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.zeros(1,3,28,28)\n",
    "t2 = torch.zeros(1,3,28,28)\n",
    "t3 = torch.zeros(1,3,28,28)\n",
    "torch.cat(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Images With An Existing Batch\n",
    "Suppose we have the same **three separate image tensors**. Only, this time, we already have **a `batch` tensor**. Assume our task is to join these three separate images with the batch.\n",
    "\n",
    "*Do we concat or do we stack?*\n",
    "\n",
    "Well, notice how the **batch axis already exists** inside the `batch` tensor. However, for the **images**, there is **no batch axis** in existence. This means neither of these will work. To join with stack or cat, we need the tensors to have **matching shapes**. So then, are we stuck? Is this impossible?\n",
    "\n",
    "It is indeed possible. It’s actually a very common task. The answer is to **first stack** and **then to concat**.\n",
    "\n",
    "We **first stack** the **three image** tensors with respect to the first dimension. This creates a new batch dimension of length three. Then, we can **concat** this new tensor with the `batch` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "batch = torch.zeros(3,3,28,28)\n",
    "t1 = torch.zeros(3,28,28)\n",
    "t2 = torch.zeros(3,28,28)\n",
    "t3 = torch.zeros(3,28,28)\n",
    "\n",
    "torch.cat(\n",
    "    (\n",
    "        batch\n",
    "        ,torch.stack(\n",
    "            (t1,t2,t3)\n",
    "            ,dim=0\n",
    "        )\n",
    "    )\n",
    "    ,dim=0\n",
    ").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the same way:\n",
    "import torch\n",
    "batch = torch.zeros(3,3,28,28)\n",
    "t1 = torch.zeros(3,28,28)\n",
    "t2 = torch.zeros(3,28,28)\n",
    "t3 = torch.zeros(3,28,28)\n",
    "\n",
    "torch.cat(\n",
    "    (\n",
    "        batch\n",
    "        ,t1.unsqueeze(0)\n",
    "        ,t2.unsqueeze(0)\n",
    "        ,t3.unsqueeze(0)\n",
    "    )\n",
    "    ,dim=0\n",
    ").shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 04\n",
    "Q1:Concatenating a sequence of tensors joins them along _______________.  \n",
    "A1:an existing axis\n",
    "\n",
    "Q2:Stacking a sequence of tensors joins them along _______________.  \n",
    "A2:a new axis\n",
    "\n",
    "Q3:What will be the output of the code below?  \n",
    "```python\n",
    "t1 = torch.tensor([1,1,1])\n",
    "t1.unsqueeze(dim=0)\n",
    "```\n",
    "A3:`tensor([[1, 1, 1]])`\n",
    "\n",
    "Q4:In PyTorch, we concatenate using which function?  \n",
    "A4:`cat()`\n",
    "\n",
    "Q5:In NumPy, we concatenate using which function?  \n",
    "A5:`concatenate()`\n",
    "\n",
    "Q6:Suppose we have the three images below. Assume our task is to join these images into a single batch. How is this done in PyTorch?  \n",
    "```python\n",
    "t1 = torch.zeros(3,28,28)\n",
    "t2 = torch.zeros(3,28,28)\n",
    "t3 = torch.zeros(3,28,28)\n",
    "```\n",
    "A6:`torch.stack( (t1,t2,t3), dim=0 )`\n",
    "\n",
    "Q7:Suppose we have the three single image batches below. Assume our task is to join these batches into a single batch. How is this done in PyTorch?\n",
    "```python\n",
    "t1 = torch.zeros(1,3,28,28)\n",
    "t2 = torch.zeros(1,3,28,28)\n",
    "t3 = torch.zeros(1,3,28,28)\n",
    "```\n",
    "A7:`torch.cat( (t1,t2,t3), dim=0 )`\n",
    "\n",
    "Q8:Suppose we have a batch with three images, and we have three single images. This is expressed in the code below. Now, assume that our task is to join the three images with the batch. What are the steps?  \n",
    "```python\n",
    "batch = torch.zeros(3,3,28,28)\n",
    "t1 = torch.zeros(3,28,28)\n",
    "t2 = torch.zeros(3,28,28)\n",
    "t3 = torch.zeros(3,28,28)\n",
    "```\n",
    "A8:stack the images then cat the result with the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
